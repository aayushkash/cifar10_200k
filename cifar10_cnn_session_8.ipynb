{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wx7pQMvNAC2q"
      },
      "outputs": [],
      "source": [
        "DROP_OUT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary libraries are installed\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torchsummary import summary\n",
        "except ModuleNotFoundError:\n",
        "    raise ModuleNotFoundError(\"The 'torch' package is not installed. Please install it using 'pip install torch'.\")\n",
        "DROP_OUT = 0.1\n",
        "class CIFAR10Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CIFAR10Net, self).__init__()\n",
        "\n",
        "        # Block 1 (Receptive Field: 3 -> 7 with dilation)\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, dilation=1, padding=1, groups=1),  # RF: 3\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, dilation=2, padding=2, groups=32),  # RF: 7 (Depthwise)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(32, 32, kernel_size=1),  # Pointwise convolution (RF unchanged: 7)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 2 (Receptive Field: 7 -> 15 -> 23)\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, dilation=2, padding=2, groups=32),  # RF: 15 (Depthwise)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, dilation=3, padding=3, groups=64),  # RF: 23 (Depthwise)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(64, 64, kernel_size=1),  # Pointwise convolution (RF unchanged: 23)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 3 (Receptive Field: 23 -> 39 -> 55)\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, dilation=3, padding=3, groups=64),  # RF: 39 (Depthwise)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, dilation=4, padding=4, groups=128),  # RF: 55 (Depthwise)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(128, 128, kernel_size=1),  # Pointwise convolution (RF unchanged: 55)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 4 (Receptive Field: 55 -> 87 -> 119)\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, dilation=4, padding=4, groups=128),  # RF: 87 (Depthwise)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, dilation=5, padding=5, groups=256),  # RF: 119 (Depthwise)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROP_OUT),\n",
        "            nn.Conv2d(256, 256, kernel_size=1),  # Pointwise convolution (RF unchanged: 119)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1)  # Global pooling (RF covers entire input)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(DROP_OUT)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uVVlTPmeAQqW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class CIFAR10Dataset:\n",
        "    def __init__(self, root=\"./data\", train=True, transform=None):\n",
        "        self.dataset = datasets.CIFAR10(root=root, train=train, download=True)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Calculate mean and std\n",
        "        if train:\n",
        "            data = np.array(self.dataset.data, dtype=np.float32)\n",
        "            self.mean = data.mean(axis=(0,1,2))/255.0\n",
        "            self.std = data.std(axis=(0,1,2))/255.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "\n",
        "        # Convert PIL Image to numpy array\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img, train=self.dataset.train)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "VhSXdKMeAaIm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "\n",
        "class Transforms:\n",
        "    def __init__(self, means, stds):\n",
        "        # Convert numpy arrays to lists if necessary\n",
        "        if isinstance(means, np.ndarray):\n",
        "            means = means.tolist()\n",
        "        if isinstance(stds, np.ndarray):\n",
        "            stds = stds.tolist()\n",
        "\n",
        "        self.train_transform = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "            A.CoarseDropout(\n",
        "                max_holes=1, max_height=16, max_width=16,\n",
        "                min_holes=1, min_height=16, min_width=16,\n",
        "                fill_value=[x * 255 for x in means],  # Convert to 0-255 range\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.Normalize(mean=means, std=stds),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "        self.test_transform = A.Compose([\n",
        "            A.Normalize(mean=means, std=stds),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img, train=True):\n",
        "        # Convert PIL Image to numpy array if needed\n",
        "        if not isinstance(img, np.ndarray):\n",
        "            img = np.array(img)\n",
        "\n",
        "        if train:\n",
        "            return self.train_transform(image=img)[\"image\"]\n",
        "        return self.test_transform(image=img)[\"image\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJOaUph-AfkN",
        "outputId": "a3837b3c-4eb1-4e46-cca5-f3b7799a71c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
        "                  f'Loss: {train_loss/(batch_idx+1):.6f} '\n",
        "                  f'Accuracy: {100.*correct/total:.2f}%')\n",
        "\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
        "          f'Accuracy: {correct}/{total} ({accuracy:.2f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h7gklqoaAjlI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CIFAR10Dataset(train=True)\n",
        "transform = Transforms(means=train_dataset.mean, stds=train_dataset.std)\n",
        "\n",
        "train_dataset.transform = transform\n",
        "test_dataset = CIFAR10Dataset(train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "model = CIFAR10Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Print model summary\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS4HrIWuAnKo",
        "outputId": "f6e82037-3cda-49ee-bea2-6725df66b259"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "           Dropout-4           [-1, 32, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]             320\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "              ReLU-7           [-1, 32, 32, 32]               0\n",
            "           Dropout-8           [-1, 32, 32, 32]               0\n",
            "            Conv2d-9           [-1, 32, 32, 32]           1,056\n",
            "      BatchNorm2d-10           [-1, 32, 32, 32]              64\n",
            "             ReLU-11           [-1, 32, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 32, 32]             640\n",
            "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
            "             ReLU-14           [-1, 64, 32, 32]               0\n",
            "          Dropout-15           [-1, 64, 32, 32]               0\n",
            "           Conv2d-16           [-1, 64, 32, 32]             640\n",
            "      BatchNorm2d-17           [-1, 64, 32, 32]             128\n",
            "             ReLU-18           [-1, 64, 32, 32]               0\n",
            "          Dropout-19           [-1, 64, 32, 32]               0\n",
            "           Conv2d-20           [-1, 64, 32, 32]           4,160\n",
            "      BatchNorm2d-21           [-1, 64, 32, 32]             128\n",
            "             ReLU-22           [-1, 64, 32, 32]               0\n",
            "           Conv2d-23          [-1, 128, 32, 32]           1,280\n",
            "      BatchNorm2d-24          [-1, 128, 32, 32]             256\n",
            "             ReLU-25          [-1, 128, 32, 32]               0\n",
            "          Dropout-26          [-1, 128, 32, 32]               0\n",
            "           Conv2d-27          [-1, 128, 32, 32]           1,280\n",
            "      BatchNorm2d-28          [-1, 128, 32, 32]             256\n",
            "             ReLU-29          [-1, 128, 32, 32]               0\n",
            "          Dropout-30          [-1, 128, 32, 32]               0\n",
            "           Conv2d-31          [-1, 128, 32, 32]          16,512\n",
            "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
            "             ReLU-33          [-1, 128, 32, 32]               0\n",
            "           Conv2d-34          [-1, 256, 32, 32]           2,560\n",
            "      BatchNorm2d-35          [-1, 256, 32, 32]             512\n",
            "             ReLU-36          [-1, 256, 32, 32]               0\n",
            "          Dropout-37          [-1, 256, 32, 32]               0\n",
            "           Conv2d-38          [-1, 256, 32, 32]           2,560\n",
            "      BatchNorm2d-39          [-1, 256, 32, 32]             512\n",
            "             ReLU-40          [-1, 256, 32, 32]               0\n",
            "          Dropout-41          [-1, 256, 32, 32]               0\n",
            "           Conv2d-42          [-1, 256, 32, 32]          65,792\n",
            "      BatchNorm2d-43          [-1, 256, 32, 32]             512\n",
            "             ReLU-44          [-1, 256, 32, 32]               0\n",
            "AdaptiveAvgPool2d-45            [-1, 256, 1, 1]               0\n",
            "           Linear-46                  [-1, 128]          32,896\n",
            "          Dropout-47                  [-1, 128]               0\n",
            "           Linear-48                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 134,762\n",
            "Trainable params: 134,762\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 41.25\n",
            "Params size (MB): 0.51\n",
            "Estimated Total Size (MB): 41.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "best_acc = 0\n",
        "for epoch in range(1, 100):\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    accuracy = test(model, device, test_loader, criterion)\n",
        "\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeevWFUNA0cV",
        "outputId": "79008b79-f652-43a2-b6db-9f30ea1d917b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 [0/50000 (0%)]\tLoss: 1.629006 Accuracy: 41.41%\n",
            "Epoch: 1 [12800/50000 (26%)]\tLoss: 1.583827 Accuracy: 41.21%\n",
            "Epoch: 1 [25600/50000 (51%)]\tLoss: 1.536960 Accuracy: 42.88%\n",
            "Epoch: 1 [38400/50000 (77%)]\tLoss: 1.494583 Accuracy: 44.67%\n",
            "\n",
            "Test set: Average loss: 1.5296, Accuracy: 4859/10000 (48.59%)\n",
            "\n",
            "Epoch: 2 [0/50000 (0%)]\tLoss: 1.356515 Accuracy: 50.78%\n",
            "Epoch: 2 [12800/50000 (26%)]\tLoss: 1.320445 Accuracy: 51.81%\n",
            "Epoch: 2 [25600/50000 (51%)]\tLoss: 1.304898 Accuracy: 52.41%\n",
            "Epoch: 2 [38400/50000 (77%)]\tLoss: 1.289786 Accuracy: 53.05%\n",
            "\n",
            "Test set: Average loss: 1.1982, Accuracy: 5826/10000 (58.26%)\n",
            "\n",
            "Epoch: 3 [0/50000 (0%)]\tLoss: 1.471847 Accuracy: 42.19%\n",
            "Epoch: 3 [12800/50000 (26%)]\tLoss: 1.203111 Accuracy: 56.03%\n",
            "Epoch: 3 [25600/50000 (51%)]\tLoss: 1.191463 Accuracy: 56.53%\n",
            "Epoch: 3 [38400/50000 (77%)]\tLoss: 1.180886 Accuracy: 57.00%\n",
            "\n",
            "Test set: Average loss: 1.1542, Accuracy: 6011/10000 (60.11%)\n",
            "\n",
            "Epoch: 4 [0/50000 (0%)]\tLoss: 1.082267 Accuracy: 63.28%\n",
            "Epoch: 4 [12800/50000 (26%)]\tLoss: 1.137735 Accuracy: 58.74%\n",
            "Epoch: 4 [25600/50000 (51%)]\tLoss: 1.118331 Accuracy: 59.57%\n",
            "Epoch: 4 [38400/50000 (77%)]\tLoss: 1.107032 Accuracy: 60.18%\n",
            "\n",
            "Test set: Average loss: 0.9915, Accuracy: 6598/10000 (65.98%)\n",
            "\n",
            "Epoch: 5 [0/50000 (0%)]\tLoss: 1.161660 Accuracy: 60.94%\n",
            "Epoch: 5 [12800/50000 (26%)]\tLoss: 1.042353 Accuracy: 62.59%\n",
            "Epoch: 5 [25600/50000 (51%)]\tLoss: 1.042710 Accuracy: 62.85%\n",
            "Epoch: 5 [38400/50000 (77%)]\tLoss: 1.039786 Accuracy: 62.93%\n",
            "\n",
            "Test set: Average loss: 0.9894, Accuracy: 6661/10000 (66.61%)\n",
            "\n",
            "Epoch: 6 [0/50000 (0%)]\tLoss: 1.111780 Accuracy: 60.94%\n",
            "Epoch: 6 [12800/50000 (26%)]\tLoss: 0.999828 Accuracy: 64.46%\n",
            "Epoch: 6 [25600/50000 (51%)]\tLoss: 0.999263 Accuracy: 64.22%\n",
            "Epoch: 6 [38400/50000 (77%)]\tLoss: 0.997659 Accuracy: 64.34%\n",
            "\n",
            "Test set: Average loss: 0.9014, Accuracy: 6884/10000 (68.84%)\n",
            "\n",
            "Epoch: 7 [0/50000 (0%)]\tLoss: 0.872763 Accuracy: 66.41%\n",
            "Epoch: 7 [12800/50000 (26%)]\tLoss: 0.978881 Accuracy: 64.65%\n",
            "Epoch: 7 [25600/50000 (51%)]\tLoss: 0.971948 Accuracy: 65.14%\n",
            "Epoch: 7 [38400/50000 (77%)]\tLoss: 0.961501 Accuracy: 65.67%\n",
            "\n",
            "Test set: Average loss: 0.9547, Accuracy: 6804/10000 (68.04%)\n",
            "\n",
            "Epoch: 8 [0/50000 (0%)]\tLoss: 0.794345 Accuracy: 73.44%\n",
            "Epoch: 8 [12800/50000 (26%)]\tLoss: 0.935543 Accuracy: 66.85%\n",
            "Epoch: 8 [25600/50000 (51%)]\tLoss: 0.931327 Accuracy: 67.00%\n",
            "Epoch: 8 [38400/50000 (77%)]\tLoss: 0.931536 Accuracy: 67.06%\n",
            "\n",
            "Test set: Average loss: 0.8038, Accuracy: 7213/10000 (72.13%)\n",
            "\n",
            "Epoch: 9 [0/50000 (0%)]\tLoss: 0.774435 Accuracy: 71.88%\n",
            "Epoch: 9 [12800/50000 (26%)]\tLoss: 0.897795 Accuracy: 68.19%\n",
            "Epoch: 9 [25600/50000 (51%)]\tLoss: 0.900025 Accuracy: 68.11%\n",
            "Epoch: 9 [38400/50000 (77%)]\tLoss: 0.900741 Accuracy: 68.09%\n",
            "\n",
            "Test set: Average loss: 0.8134, Accuracy: 7240/10000 (72.40%)\n",
            "\n",
            "Epoch: 10 [0/50000 (0%)]\tLoss: 0.829394 Accuracy: 67.97%\n",
            "Epoch: 10 [12800/50000 (26%)]\tLoss: 0.877489 Accuracy: 69.05%\n",
            "Epoch: 10 [25600/50000 (51%)]\tLoss: 0.880097 Accuracy: 68.72%\n",
            "Epoch: 10 [38400/50000 (77%)]\tLoss: 0.877371 Accuracy: 68.89%\n",
            "\n",
            "Test set: Average loss: 0.7617, Accuracy: 7388/10000 (73.88%)\n",
            "\n",
            "Epoch: 11 [0/50000 (0%)]\tLoss: 1.045996 Accuracy: 66.41%\n",
            "Epoch: 11 [12800/50000 (26%)]\tLoss: 0.874703 Accuracy: 69.52%\n",
            "Epoch: 11 [25600/50000 (51%)]\tLoss: 0.862963 Accuracy: 69.92%\n",
            "Epoch: 11 [38400/50000 (77%)]\tLoss: 0.863235 Accuracy: 69.75%\n",
            "\n",
            "Test set: Average loss: 0.7932, Accuracy: 7376/10000 (73.76%)\n",
            "\n",
            "Epoch: 12 [0/50000 (0%)]\tLoss: 0.758039 Accuracy: 74.22%\n",
            "Epoch: 12 [12800/50000 (26%)]\tLoss: 0.857948 Accuracy: 69.53%\n",
            "Epoch: 12 [25600/50000 (51%)]\tLoss: 0.857072 Accuracy: 69.82%\n",
            "Epoch: 12 [38400/50000 (77%)]\tLoss: 0.855254 Accuracy: 69.85%\n",
            "\n",
            "Test set: Average loss: 0.7668, Accuracy: 7445/10000 (74.45%)\n",
            "\n",
            "Epoch: 13 [0/50000 (0%)]\tLoss: 0.800283 Accuracy: 75.00%\n",
            "Epoch: 13 [12800/50000 (26%)]\tLoss: 0.840906 Accuracy: 70.22%\n",
            "Epoch: 13 [25600/50000 (51%)]\tLoss: 0.833568 Accuracy: 70.63%\n",
            "Epoch: 13 [38400/50000 (77%)]\tLoss: 0.833604 Accuracy: 70.76%\n",
            "\n",
            "Test set: Average loss: 0.7478, Accuracy: 7527/10000 (75.27%)\n",
            "\n",
            "Epoch: 14 [0/50000 (0%)]\tLoss: 0.823309 Accuracy: 70.31%\n",
            "Epoch: 14 [12800/50000 (26%)]\tLoss: 0.801930 Accuracy: 71.70%\n",
            "Epoch: 14 [25600/50000 (51%)]\tLoss: 0.810667 Accuracy: 71.33%\n",
            "Epoch: 14 [38400/50000 (77%)]\tLoss: 0.813134 Accuracy: 71.11%\n",
            "\n",
            "Test set: Average loss: 0.7314, Accuracy: 7568/10000 (75.68%)\n",
            "\n",
            "Epoch: 15 [0/50000 (0%)]\tLoss: 0.688071 Accuracy: 75.78%\n",
            "Epoch: 15 [12800/50000 (26%)]\tLoss: 0.793905 Accuracy: 72.05%\n",
            "Epoch: 15 [25600/50000 (51%)]\tLoss: 0.803123 Accuracy: 71.82%\n",
            "Epoch: 15 [38400/50000 (77%)]\tLoss: 0.795919 Accuracy: 72.07%\n",
            "\n",
            "Test set: Average loss: 0.6878, Accuracy: 7654/10000 (76.54%)\n",
            "\n",
            "Epoch: 16 [0/50000 (0%)]\tLoss: 0.854378 Accuracy: 67.97%\n",
            "Epoch: 16 [12800/50000 (26%)]\tLoss: 0.794687 Accuracy: 72.08%\n",
            "Epoch: 16 [25600/50000 (51%)]\tLoss: 0.792209 Accuracy: 72.08%\n",
            "Epoch: 16 [38400/50000 (77%)]\tLoss: 0.789985 Accuracy: 72.13%\n",
            "\n",
            "Test set: Average loss: 0.7614, Accuracy: 7510/10000 (75.10%)\n",
            "\n",
            "Epoch: 17 [0/50000 (0%)]\tLoss: 0.694720 Accuracy: 77.34%\n",
            "Epoch: 17 [12800/50000 (26%)]\tLoss: 0.786668 Accuracy: 72.51%\n",
            "Epoch: 17 [25600/50000 (51%)]\tLoss: 0.789740 Accuracy: 72.26%\n",
            "Epoch: 17 [38400/50000 (77%)]\tLoss: 0.783509 Accuracy: 72.66%\n",
            "\n",
            "Test set: Average loss: 0.6822, Accuracy: 7787/10000 (77.87%)\n",
            "\n",
            "Epoch: 18 [0/50000 (0%)]\tLoss: 0.799222 Accuracy: 74.22%\n",
            "Epoch: 18 [12800/50000 (26%)]\tLoss: 0.773645 Accuracy: 72.83%\n",
            "Epoch: 18 [25600/50000 (51%)]\tLoss: 0.775917 Accuracy: 72.74%\n",
            "Epoch: 18 [38400/50000 (77%)]\tLoss: 0.772574 Accuracy: 72.89%\n",
            "\n",
            "Test set: Average loss: 0.6966, Accuracy: 7693/10000 (76.93%)\n",
            "\n",
            "Epoch: 19 [0/50000 (0%)]\tLoss: 0.768398 Accuracy: 74.22%\n",
            "Epoch: 19 [12800/50000 (26%)]\tLoss: 0.764774 Accuracy: 73.29%\n",
            "Epoch: 19 [25600/50000 (51%)]\tLoss: 0.762507 Accuracy: 73.25%\n",
            "Epoch: 19 [38400/50000 (77%)]\tLoss: 0.764433 Accuracy: 73.32%\n",
            "\n",
            "Test set: Average loss: 0.6213, Accuracy: 7932/10000 (79.32%)\n",
            "\n",
            "Epoch: 20 [0/50000 (0%)]\tLoss: 0.751615 Accuracy: 72.66%\n",
            "Epoch: 20 [12800/50000 (26%)]\tLoss: 0.747541 Accuracy: 73.98%\n",
            "Epoch: 20 [25600/50000 (51%)]\tLoss: 0.753417 Accuracy: 73.85%\n",
            "Epoch: 20 [38400/50000 (77%)]\tLoss: 0.751446 Accuracy: 73.73%\n",
            "\n",
            "Test set: Average loss: 0.6116, Accuracy: 7941/10000 (79.41%)\n",
            "\n",
            "Epoch: 21 [0/50000 (0%)]\tLoss: 0.707607 Accuracy: 75.00%\n",
            "Epoch: 21 [12800/50000 (26%)]\tLoss: 0.733546 Accuracy: 74.34%\n",
            "Epoch: 21 [25600/50000 (51%)]\tLoss: 0.740206 Accuracy: 74.21%\n",
            "Epoch: 21 [38400/50000 (77%)]\tLoss: 0.739628 Accuracy: 74.25%\n",
            "\n",
            "Test set: Average loss: 0.5962, Accuracy: 8020/10000 (80.20%)\n",
            "\n",
            "Epoch: 22 [0/50000 (0%)]\tLoss: 0.841910 Accuracy: 71.09%\n",
            "Epoch: 22 [12800/50000 (26%)]\tLoss: 0.719254 Accuracy: 74.85%\n",
            "Epoch: 22 [25600/50000 (51%)]\tLoss: 0.729067 Accuracy: 74.56%\n",
            "Epoch: 22 [38400/50000 (77%)]\tLoss: 0.727861 Accuracy: 74.52%\n",
            "\n",
            "Test set: Average loss: 0.6408, Accuracy: 7910/10000 (79.10%)\n",
            "\n",
            "Epoch: 23 [0/50000 (0%)]\tLoss: 0.682169 Accuracy: 72.66%\n",
            "Epoch: 23 [12800/50000 (26%)]\tLoss: 0.714023 Accuracy: 74.71%\n",
            "Epoch: 23 [25600/50000 (51%)]\tLoss: 0.717562 Accuracy: 74.71%\n",
            "Epoch: 23 [38400/50000 (77%)]\tLoss: 0.720176 Accuracy: 74.61%\n",
            "\n",
            "Test set: Average loss: 0.6124, Accuracy: 7963/10000 (79.63%)\n",
            "\n",
            "Epoch: 24 [0/50000 (0%)]\tLoss: 0.666124 Accuracy: 77.34%\n",
            "Epoch: 24 [12800/50000 (26%)]\tLoss: 0.737564 Accuracy: 74.19%\n",
            "Epoch: 24 [25600/50000 (51%)]\tLoss: 0.723878 Accuracy: 74.46%\n",
            "Epoch: 24 [38400/50000 (77%)]\tLoss: 0.719567 Accuracy: 74.57%\n",
            "\n",
            "Test set: Average loss: 0.5982, Accuracy: 8013/10000 (80.13%)\n",
            "\n",
            "Epoch: 25 [0/50000 (0%)]\tLoss: 0.716024 Accuracy: 74.22%\n",
            "Epoch: 25 [12800/50000 (26%)]\tLoss: 0.708545 Accuracy: 75.12%\n",
            "Epoch: 25 [25600/50000 (51%)]\tLoss: 0.710057 Accuracy: 75.04%\n",
            "Epoch: 25 [38400/50000 (77%)]\tLoss: 0.709020 Accuracy: 74.99%\n",
            "\n",
            "Test set: Average loss: 0.6063, Accuracy: 8015/10000 (80.15%)\n",
            "\n",
            "Epoch: 26 [0/50000 (0%)]\tLoss: 0.687125 Accuracy: 78.12%\n",
            "Epoch: 26 [12800/50000 (26%)]\tLoss: 0.704159 Accuracy: 75.01%\n",
            "Epoch: 26 [25600/50000 (51%)]\tLoss: 0.698434 Accuracy: 75.48%\n",
            "Epoch: 26 [38400/50000 (77%)]\tLoss: 0.698232 Accuracy: 75.51%\n",
            "\n",
            "Test set: Average loss: 0.5799, Accuracy: 8106/10000 (81.06%)\n",
            "\n",
            "Epoch: 27 [0/50000 (0%)]\tLoss: 0.562675 Accuracy: 80.47%\n",
            "Epoch: 27 [12800/50000 (26%)]\tLoss: 0.698887 Accuracy: 75.55%\n",
            "Epoch: 27 [25600/50000 (51%)]\tLoss: 0.694413 Accuracy: 75.70%\n",
            "Epoch: 27 [38400/50000 (77%)]\tLoss: 0.698671 Accuracy: 75.43%\n",
            "\n",
            "Test set: Average loss: 0.5838, Accuracy: 8096/10000 (80.96%)\n",
            "\n",
            "Epoch: 28 [0/50000 (0%)]\tLoss: 0.733471 Accuracy: 73.44%\n",
            "Epoch: 28 [12800/50000 (26%)]\tLoss: 0.699046 Accuracy: 75.44%\n",
            "Epoch: 28 [25600/50000 (51%)]\tLoss: 0.697802 Accuracy: 75.63%\n",
            "Epoch: 28 [38400/50000 (77%)]\tLoss: 0.698289 Accuracy: 75.77%\n",
            "\n",
            "Test set: Average loss: 0.5636, Accuracy: 8147/10000 (81.47%)\n",
            "\n",
            "Epoch: 29 [0/50000 (0%)]\tLoss: 0.627658 Accuracy: 78.12%\n",
            "Epoch: 29 [12800/50000 (26%)]\tLoss: 0.675250 Accuracy: 75.94%\n",
            "Epoch: 29 [25600/50000 (51%)]\tLoss: 0.687449 Accuracy: 75.59%\n",
            "Epoch: 29 [38400/50000 (77%)]\tLoss: 0.685992 Accuracy: 75.67%\n",
            "\n",
            "Test set: Average loss: 0.5502, Accuracy: 8159/10000 (81.59%)\n",
            "\n",
            "Epoch: 30 [0/50000 (0%)]\tLoss: 0.631058 Accuracy: 77.34%\n",
            "Epoch: 30 [12800/50000 (26%)]\tLoss: 0.672621 Accuracy: 76.51%\n",
            "Epoch: 30 [25600/50000 (51%)]\tLoss: 0.677641 Accuracy: 76.47%\n",
            "Epoch: 30 [38400/50000 (77%)]\tLoss: 0.680978 Accuracy: 76.21%\n",
            "\n",
            "Test set: Average loss: 0.5493, Accuracy: 8146/10000 (81.46%)\n",
            "\n",
            "Epoch: 31 [0/50000 (0%)]\tLoss: 0.672405 Accuracy: 72.66%\n",
            "Epoch: 31 [12800/50000 (26%)]\tLoss: 0.666369 Accuracy: 76.14%\n",
            "Epoch: 31 [25600/50000 (51%)]\tLoss: 0.669779 Accuracy: 76.36%\n",
            "Epoch: 31 [38400/50000 (77%)]\tLoss: 0.675285 Accuracy: 76.14%\n",
            "\n",
            "Test set: Average loss: 0.5620, Accuracy: 8154/10000 (81.54%)\n",
            "\n",
            "Epoch: 32 [0/50000 (0%)]\tLoss: 0.676985 Accuracy: 76.56%\n",
            "Epoch: 32 [12800/50000 (26%)]\tLoss: 0.656998 Accuracy: 76.90%\n",
            "Epoch: 32 [25600/50000 (51%)]\tLoss: 0.660046 Accuracy: 76.55%\n",
            "Epoch: 32 [38400/50000 (77%)]\tLoss: 0.663412 Accuracy: 76.65%\n",
            "\n",
            "Test set: Average loss: 0.6199, Accuracy: 7984/10000 (79.84%)\n",
            "\n",
            "Epoch: 33 [0/50000 (0%)]\tLoss: 0.733880 Accuracy: 70.31%\n",
            "Epoch: 33 [12800/50000 (26%)]\tLoss: 0.666418 Accuracy: 76.51%\n",
            "Epoch: 33 [25600/50000 (51%)]\tLoss: 0.668117 Accuracy: 76.72%\n",
            "Epoch: 33 [38400/50000 (77%)]\tLoss: 0.668390 Accuracy: 76.66%\n",
            "\n",
            "Test set: Average loss: 0.5696, Accuracy: 8137/10000 (81.37%)\n",
            "\n",
            "Epoch: 34 [0/50000 (0%)]\tLoss: 0.583104 Accuracy: 77.34%\n",
            "Epoch: 34 [12800/50000 (26%)]\tLoss: 0.663133 Accuracy: 76.75%\n",
            "Epoch: 34 [25600/50000 (51%)]\tLoss: 0.668536 Accuracy: 76.42%\n",
            "Epoch: 34 [38400/50000 (77%)]\tLoss: 0.662909 Accuracy: 76.68%\n",
            "\n",
            "Test set: Average loss: 0.5606, Accuracy: 8135/10000 (81.35%)\n",
            "\n",
            "Epoch: 35 [0/50000 (0%)]\tLoss: 0.659006 Accuracy: 75.00%\n",
            "Epoch: 35 [12800/50000 (26%)]\tLoss: 0.653389 Accuracy: 77.37%\n",
            "Epoch: 35 [25600/50000 (51%)]\tLoss: 0.656727 Accuracy: 76.98%\n",
            "Epoch: 35 [38400/50000 (77%)]\tLoss: 0.657552 Accuracy: 76.92%\n",
            "\n",
            "Test set: Average loss: 0.5816, Accuracy: 8104/10000 (81.04%)\n",
            "\n",
            "Epoch: 36 [0/50000 (0%)]\tLoss: 0.753467 Accuracy: 75.00%\n",
            "Epoch: 36 [12800/50000 (26%)]\tLoss: 0.647361 Accuracy: 77.23%\n",
            "Epoch: 36 [25600/50000 (51%)]\tLoss: 0.649638 Accuracy: 77.27%\n",
            "Epoch: 36 [38400/50000 (77%)]\tLoss: 0.651433 Accuracy: 77.16%\n",
            "\n",
            "Test set: Average loss: 0.5698, Accuracy: 8149/10000 (81.49%)\n",
            "\n",
            "Epoch: 37 [0/50000 (0%)]\tLoss: 0.727539 Accuracy: 72.66%\n",
            "Epoch: 37 [12800/50000 (26%)]\tLoss: 0.657214 Accuracy: 76.85%\n",
            "Epoch: 37 [25600/50000 (51%)]\tLoss: 0.650091 Accuracy: 77.16%\n",
            "Epoch: 37 [38400/50000 (77%)]\tLoss: 0.652512 Accuracy: 77.12%\n",
            "\n",
            "Test set: Average loss: 0.5710, Accuracy: 8161/10000 (81.61%)\n",
            "\n",
            "Epoch: 38 [0/50000 (0%)]\tLoss: 0.644658 Accuracy: 75.78%\n",
            "Epoch: 38 [12800/50000 (26%)]\tLoss: 0.640191 Accuracy: 77.48%\n",
            "Epoch: 38 [25600/50000 (51%)]\tLoss: 0.643034 Accuracy: 77.41%\n",
            "Epoch: 38 [38400/50000 (77%)]\tLoss: 0.642876 Accuracy: 77.46%\n",
            "\n",
            "Test set: Average loss: 0.5556, Accuracy: 8194/10000 (81.94%)\n",
            "\n",
            "Epoch: 39 [0/50000 (0%)]\tLoss: 0.589425 Accuracy: 77.34%\n",
            "Epoch: 39 [12800/50000 (26%)]\tLoss: 0.627962 Accuracy: 77.77%\n",
            "Epoch: 39 [25600/50000 (51%)]\tLoss: 0.634799 Accuracy: 77.63%\n",
            "Epoch: 39 [38400/50000 (77%)]\tLoss: 0.634890 Accuracy: 77.55%\n",
            "\n",
            "Test set: Average loss: 0.5633, Accuracy: 8158/10000 (81.58%)\n",
            "\n",
            "Epoch: 40 [0/50000 (0%)]\tLoss: 0.862695 Accuracy: 64.84%\n",
            "Epoch: 40 [12800/50000 (26%)]\tLoss: 0.629243 Accuracy: 77.55%\n",
            "Epoch: 40 [25600/50000 (51%)]\tLoss: 0.634705 Accuracy: 77.60%\n",
            "Epoch: 40 [38400/50000 (77%)]\tLoss: 0.638906 Accuracy: 77.65%\n",
            "\n",
            "Test set: Average loss: 0.5508, Accuracy: 8223/10000 (82.23%)\n",
            "\n",
            "Epoch: 41 [0/50000 (0%)]\tLoss: 0.557479 Accuracy: 79.69%\n",
            "Epoch: 41 [12800/50000 (26%)]\tLoss: 0.614867 Accuracy: 78.77%\n",
            "Epoch: 41 [25600/50000 (51%)]\tLoss: 0.633729 Accuracy: 78.02%\n",
            "Epoch: 41 [38400/50000 (77%)]\tLoss: 0.633541 Accuracy: 77.90%\n",
            "\n",
            "Test set: Average loss: 0.5322, Accuracy: 8301/10000 (83.01%)\n",
            "\n",
            "Epoch: 42 [0/50000 (0%)]\tLoss: 0.549084 Accuracy: 82.81%\n",
            "Epoch: 42 [12800/50000 (26%)]\tLoss: 0.631496 Accuracy: 78.02%\n",
            "Epoch: 42 [25600/50000 (51%)]\tLoss: 0.636579 Accuracy: 77.84%\n",
            "Epoch: 42 [38400/50000 (77%)]\tLoss: 0.631504 Accuracy: 77.98%\n",
            "\n",
            "Test set: Average loss: 0.5513, Accuracy: 8217/10000 (82.17%)\n",
            "\n",
            "Epoch: 43 [0/50000 (0%)]\tLoss: 0.577256 Accuracy: 77.34%\n",
            "Epoch: 43 [12800/50000 (26%)]\tLoss: 0.635575 Accuracy: 78.12%\n",
            "Epoch: 43 [25600/50000 (51%)]\tLoss: 0.631982 Accuracy: 77.96%\n",
            "Epoch: 43 [38400/50000 (77%)]\tLoss: 0.630690 Accuracy: 78.10%\n",
            "\n",
            "Test set: Average loss: 0.5450, Accuracy: 8232/10000 (82.32%)\n",
            "\n",
            "Epoch: 44 [0/50000 (0%)]\tLoss: 0.600134 Accuracy: 75.78%\n",
            "Epoch: 44 [12800/50000 (26%)]\tLoss: 0.616204 Accuracy: 78.33%\n",
            "Epoch: 44 [25600/50000 (51%)]\tLoss: 0.612886 Accuracy: 78.42%\n",
            "Epoch: 44 [38400/50000 (77%)]\tLoss: 0.611541 Accuracy: 78.47%\n",
            "\n",
            "Test set: Average loss: 0.5182, Accuracy: 8323/10000 (83.23%)\n",
            "\n",
            "Epoch: 45 [0/50000 (0%)]\tLoss: 0.488958 Accuracy: 81.25%\n",
            "Epoch: 45 [12800/50000 (26%)]\tLoss: 0.614904 Accuracy: 78.67%\n",
            "Epoch: 45 [25600/50000 (51%)]\tLoss: 0.621806 Accuracy: 78.24%\n",
            "Epoch: 45 [38400/50000 (77%)]\tLoss: 0.623753 Accuracy: 78.13%\n",
            "\n",
            "Test set: Average loss: 0.5320, Accuracy: 8265/10000 (82.65%)\n",
            "\n",
            "Epoch: 46 [0/50000 (0%)]\tLoss: 0.551933 Accuracy: 82.81%\n",
            "Epoch: 46 [12800/50000 (26%)]\tLoss: 0.599092 Accuracy: 78.98%\n",
            "Epoch: 46 [25600/50000 (51%)]\tLoss: 0.608464 Accuracy: 78.63%\n",
            "Epoch: 46 [38400/50000 (77%)]\tLoss: 0.608557 Accuracy: 78.72%\n",
            "\n",
            "Test set: Average loss: 0.4965, Accuracy: 8370/10000 (83.70%)\n",
            "\n",
            "Epoch: 47 [0/50000 (0%)]\tLoss: 0.616128 Accuracy: 80.47%\n",
            "Epoch: 47 [12800/50000 (26%)]\tLoss: 0.606325 Accuracy: 78.94%\n",
            "Epoch: 47 [25600/50000 (51%)]\tLoss: 0.607236 Accuracy: 78.76%\n",
            "Epoch: 47 [38400/50000 (77%)]\tLoss: 0.608302 Accuracy: 78.73%\n",
            "\n",
            "Test set: Average loss: 0.5084, Accuracy: 8314/10000 (83.14%)\n",
            "\n",
            "Epoch: 48 [0/50000 (0%)]\tLoss: 0.714921 Accuracy: 73.44%\n",
            "Epoch: 48 [12800/50000 (26%)]\tLoss: 0.611613 Accuracy: 78.72%\n",
            "Epoch: 48 [25600/50000 (51%)]\tLoss: 0.610814 Accuracy: 78.67%\n",
            "Epoch: 48 [38400/50000 (77%)]\tLoss: 0.606905 Accuracy: 78.78%\n",
            "\n",
            "Test set: Average loss: 0.5106, Accuracy: 8380/10000 (83.80%)\n",
            "\n",
            "Epoch: 49 [0/50000 (0%)]\tLoss: 0.650333 Accuracy: 75.00%\n",
            "Epoch: 49 [12800/50000 (26%)]\tLoss: 0.595996 Accuracy: 78.86%\n",
            "Epoch: 49 [25600/50000 (51%)]\tLoss: 0.603191 Accuracy: 78.61%\n",
            "Epoch: 49 [38400/50000 (77%)]\tLoss: 0.605049 Accuracy: 78.70%\n",
            "\n",
            "Test set: Average loss: 0.5118, Accuracy: 8303/10000 (83.03%)\n",
            "\n",
            "Epoch: 50 [0/50000 (0%)]\tLoss: 0.506249 Accuracy: 80.47%\n",
            "Epoch: 50 [12800/50000 (26%)]\tLoss: 0.584985 Accuracy: 79.59%\n",
            "Epoch: 50 [25600/50000 (51%)]\tLoss: 0.591552 Accuracy: 79.33%\n",
            "Epoch: 50 [38400/50000 (77%)]\tLoss: 0.599518 Accuracy: 78.95%\n",
            "\n",
            "Test set: Average loss: 0.5242, Accuracy: 8322/10000 (83.22%)\n",
            "\n",
            "Epoch: 51 [0/50000 (0%)]\tLoss: 0.596810 Accuracy: 82.03%\n",
            "Epoch: 51 [12800/50000 (26%)]\tLoss: 0.590797 Accuracy: 79.30%\n",
            "Epoch: 51 [25600/50000 (51%)]\tLoss: 0.599506 Accuracy: 78.93%\n",
            "Epoch: 51 [38400/50000 (77%)]\tLoss: 0.600654 Accuracy: 78.88%\n",
            "\n",
            "Test set: Average loss: 0.5147, Accuracy: 8320/10000 (83.20%)\n",
            "\n",
            "Epoch: 52 [0/50000 (0%)]\tLoss: 0.466880 Accuracy: 86.72%\n",
            "Epoch: 52 [12800/50000 (26%)]\tLoss: 0.589442 Accuracy: 79.25%\n",
            "Epoch: 52 [25600/50000 (51%)]\tLoss: 0.591424 Accuracy: 79.21%\n",
            "Epoch: 52 [38400/50000 (77%)]\tLoss: 0.589231 Accuracy: 79.31%\n",
            "\n",
            "Test set: Average loss: 0.5145, Accuracy: 8312/10000 (83.12%)\n",
            "\n",
            "Epoch: 53 [0/50000 (0%)]\tLoss: 0.569257 Accuracy: 79.69%\n",
            "Epoch: 53 [12800/50000 (26%)]\tLoss: 0.576598 Accuracy: 79.73%\n",
            "Epoch: 53 [25600/50000 (51%)]\tLoss: 0.573764 Accuracy: 79.95%\n",
            "Epoch: 53 [38400/50000 (77%)]\tLoss: 0.585690 Accuracy: 79.41%\n",
            "\n",
            "Test set: Average loss: 0.5070, Accuracy: 8375/10000 (83.75%)\n",
            "\n",
            "Epoch: 54 [0/50000 (0%)]\tLoss: 0.632502 Accuracy: 78.12%\n",
            "Epoch: 54 [12800/50000 (26%)]\tLoss: 0.582165 Accuracy: 79.67%\n",
            "Epoch: 54 [25600/50000 (51%)]\tLoss: 0.588374 Accuracy: 79.30%\n",
            "Epoch: 54 [38400/50000 (77%)]\tLoss: 0.587141 Accuracy: 79.30%\n",
            "\n",
            "Test set: Average loss: 0.5053, Accuracy: 8395/10000 (83.95%)\n",
            "\n",
            "Epoch: 55 [0/50000 (0%)]\tLoss: 0.431957 Accuracy: 83.59%\n",
            "Epoch: 55 [12800/50000 (26%)]\tLoss: 0.590575 Accuracy: 79.04%\n",
            "Epoch: 55 [25600/50000 (51%)]\tLoss: 0.587320 Accuracy: 79.41%\n",
            "Epoch: 55 [38400/50000 (77%)]\tLoss: 0.587142 Accuracy: 79.45%\n",
            "\n",
            "Test set: Average loss: 0.5151, Accuracy: 8350/10000 (83.50%)\n",
            "\n",
            "Epoch: 56 [0/50000 (0%)]\tLoss: 0.637407 Accuracy: 76.56%\n",
            "Epoch: 56 [12800/50000 (26%)]\tLoss: 0.574581 Accuracy: 79.90%\n",
            "Epoch: 56 [25600/50000 (51%)]\tLoss: 0.577615 Accuracy: 79.66%\n",
            "Epoch: 56 [38400/50000 (77%)]\tLoss: 0.577923 Accuracy: 79.70%\n",
            "\n",
            "Test set: Average loss: 0.4976, Accuracy: 8401/10000 (84.01%)\n",
            "\n",
            "Epoch: 57 [0/50000 (0%)]\tLoss: 0.518629 Accuracy: 81.25%\n",
            "Epoch: 57 [12800/50000 (26%)]\tLoss: 0.572063 Accuracy: 79.84%\n",
            "Epoch: 57 [25600/50000 (51%)]\tLoss: 0.582858 Accuracy: 79.60%\n",
            "Epoch: 57 [38400/50000 (77%)]\tLoss: 0.580584 Accuracy: 79.64%\n",
            "\n",
            "Test set: Average loss: 0.4982, Accuracy: 8400/10000 (84.00%)\n",
            "\n",
            "Epoch: 58 [0/50000 (0%)]\tLoss: 0.560201 Accuracy: 80.47%\n",
            "Epoch: 58 [12800/50000 (26%)]\tLoss: 0.569019 Accuracy: 79.82%\n",
            "Epoch: 58 [25600/50000 (51%)]\tLoss: 0.575256 Accuracy: 79.51%\n",
            "Epoch: 58 [38400/50000 (77%)]\tLoss: 0.574493 Accuracy: 79.52%\n",
            "\n",
            "Test set: Average loss: 0.5119, Accuracy: 8388/10000 (83.88%)\n",
            "\n",
            "Epoch: 59 [0/50000 (0%)]\tLoss: 0.415505 Accuracy: 85.16%\n",
            "Epoch: 59 [12800/50000 (26%)]\tLoss: 0.557729 Accuracy: 80.34%\n",
            "Epoch: 59 [25600/50000 (51%)]\tLoss: 0.566080 Accuracy: 80.16%\n",
            "Epoch: 59 [38400/50000 (77%)]\tLoss: 0.570229 Accuracy: 80.06%\n",
            "\n",
            "Test set: Average loss: 0.4914, Accuracy: 8420/10000 (84.20%)\n",
            "\n",
            "Epoch: 60 [0/50000 (0%)]\tLoss: 0.541239 Accuracy: 79.69%\n",
            "Epoch: 60 [12800/50000 (26%)]\tLoss: 0.564339 Accuracy: 80.12%\n",
            "Epoch: 60 [25600/50000 (51%)]\tLoss: 0.557725 Accuracy: 80.27%\n",
            "Epoch: 60 [38400/50000 (77%)]\tLoss: 0.566615 Accuracy: 80.22%\n",
            "\n",
            "Test set: Average loss: 0.5010, Accuracy: 8391/10000 (83.91%)\n",
            "\n",
            "Epoch: 61 [0/50000 (0%)]\tLoss: 0.501416 Accuracy: 81.25%\n",
            "Epoch: 61 [12800/50000 (26%)]\tLoss: 0.563758 Accuracy: 80.03%\n",
            "Epoch: 61 [25600/50000 (51%)]\tLoss: 0.570091 Accuracy: 79.85%\n",
            "Epoch: 61 [38400/50000 (77%)]\tLoss: 0.565587 Accuracy: 80.05%\n",
            "\n",
            "Test set: Average loss: 0.4797, Accuracy: 8431/10000 (84.31%)\n",
            "\n",
            "Epoch: 62 [0/50000 (0%)]\tLoss: 0.483387 Accuracy: 85.16%\n",
            "Epoch: 62 [12800/50000 (26%)]\tLoss: 0.563465 Accuracy: 80.05%\n",
            "Epoch: 62 [25600/50000 (51%)]\tLoss: 0.563326 Accuracy: 80.04%\n",
            "Epoch: 62 [38400/50000 (77%)]\tLoss: 0.563255 Accuracy: 79.97%\n",
            "\n",
            "Test set: Average loss: 0.4803, Accuracy: 8424/10000 (84.24%)\n",
            "\n",
            "Epoch: 63 [0/50000 (0%)]\tLoss: 0.594549 Accuracy: 79.69%\n",
            "Epoch: 63 [12800/50000 (26%)]\tLoss: 0.564926 Accuracy: 80.12%\n",
            "Epoch: 63 [25600/50000 (51%)]\tLoss: 0.569487 Accuracy: 80.18%\n",
            "Epoch: 63 [38400/50000 (77%)]\tLoss: 0.567403 Accuracy: 80.17%\n",
            "\n",
            "Test set: Average loss: 0.4770, Accuracy: 8411/10000 (84.11%)\n",
            "\n",
            "Epoch: 64 [0/50000 (0%)]\tLoss: 0.517711 Accuracy: 86.72%\n",
            "Epoch: 64 [12800/50000 (26%)]\tLoss: 0.551154 Accuracy: 80.52%\n",
            "Epoch: 64 [25600/50000 (51%)]\tLoss: 0.552295 Accuracy: 80.52%\n",
            "Epoch: 64 [38400/50000 (77%)]\tLoss: 0.553672 Accuracy: 80.38%\n",
            "\n",
            "Test set: Average loss: 0.4770, Accuracy: 8440/10000 (84.40%)\n",
            "\n",
            "Epoch: 65 [0/50000 (0%)]\tLoss: 0.551348 Accuracy: 79.69%\n",
            "Epoch: 65 [12800/50000 (26%)]\tLoss: 0.545327 Accuracy: 80.88%\n",
            "Epoch: 65 [25600/50000 (51%)]\tLoss: 0.551361 Accuracy: 80.73%\n",
            "Epoch: 65 [38400/50000 (77%)]\tLoss: 0.549213 Accuracy: 80.72%\n",
            "\n",
            "Test set: Average loss: 0.4752, Accuracy: 8460/10000 (84.60%)\n",
            "\n",
            "Epoch: 66 [0/50000 (0%)]\tLoss: 0.517505 Accuracy: 81.25%\n",
            "Epoch: 66 [12800/50000 (26%)]\tLoss: 0.537489 Accuracy: 80.74%\n",
            "Epoch: 66 [25600/50000 (51%)]\tLoss: 0.545051 Accuracy: 80.88%\n",
            "Epoch: 66 [38400/50000 (77%)]\tLoss: 0.546750 Accuracy: 80.81%\n",
            "\n",
            "Test set: Average loss: 0.4637, Accuracy: 8499/10000 (84.99%)\n",
            "\n",
            "Epoch: 67 [0/50000 (0%)]\tLoss: 0.422001 Accuracy: 82.81%\n",
            "Epoch: 67 [12800/50000 (26%)]\tLoss: 0.560259 Accuracy: 80.21%\n",
            "Epoch: 67 [25600/50000 (51%)]\tLoss: 0.552474 Accuracy: 80.37%\n",
            "Epoch: 67 [38400/50000 (77%)]\tLoss: 0.552653 Accuracy: 80.60%\n",
            "\n",
            "Test set: Average loss: 0.4811, Accuracy: 8464/10000 (84.64%)\n",
            "\n",
            "Epoch: 68 [0/50000 (0%)]\tLoss: 0.683270 Accuracy: 77.34%\n",
            "Epoch: 68 [12800/50000 (26%)]\tLoss: 0.551675 Accuracy: 80.64%\n",
            "Epoch: 68 [25600/50000 (51%)]\tLoss: 0.549919 Accuracy: 80.77%\n",
            "Epoch: 68 [38400/50000 (77%)]\tLoss: 0.548937 Accuracy: 80.79%\n",
            "\n",
            "Test set: Average loss: 0.4704, Accuracy: 8491/10000 (84.91%)\n",
            "\n",
            "Epoch: 69 [0/50000 (0%)]\tLoss: 0.462320 Accuracy: 82.03%\n",
            "Epoch: 69 [12800/50000 (26%)]\tLoss: 0.544596 Accuracy: 80.80%\n",
            "Epoch: 69 [25600/50000 (51%)]\tLoss: 0.545985 Accuracy: 80.84%\n",
            "Epoch: 69 [38400/50000 (77%)]\tLoss: 0.545662 Accuracy: 80.94%\n",
            "\n",
            "Test set: Average loss: 0.4646, Accuracy: 8509/10000 (85.09%)\n",
            "\n",
            "Epoch: 70 [0/50000 (0%)]\tLoss: 0.588324 Accuracy: 79.69%\n",
            "Epoch: 70 [12800/50000 (26%)]\tLoss: 0.536487 Accuracy: 80.85%\n",
            "Epoch: 70 [25600/50000 (51%)]\tLoss: 0.541323 Accuracy: 80.85%\n",
            "Epoch: 70 [38400/50000 (77%)]\tLoss: 0.542702 Accuracy: 80.83%\n",
            "\n",
            "Test set: Average loss: 0.4640, Accuracy: 8459/10000 (84.59%)\n",
            "\n",
            "Epoch: 71 [0/50000 (0%)]\tLoss: 0.474694 Accuracy: 85.16%\n",
            "Epoch: 71 [12800/50000 (26%)]\tLoss: 0.546476 Accuracy: 80.88%\n",
            "Epoch: 71 [25600/50000 (51%)]\tLoss: 0.549005 Accuracy: 80.87%\n",
            "Epoch: 71 [38400/50000 (77%)]\tLoss: 0.549002 Accuracy: 80.82%\n",
            "\n",
            "Test set: Average loss: 0.4757, Accuracy: 8484/10000 (84.84%)\n",
            "\n",
            "Epoch: 72 [0/50000 (0%)]\tLoss: 0.605186 Accuracy: 78.91%\n",
            "Epoch: 72 [12800/50000 (26%)]\tLoss: 0.540410 Accuracy: 81.32%\n",
            "Epoch: 72 [25600/50000 (51%)]\tLoss: 0.545864 Accuracy: 80.98%\n",
            "Epoch: 72 [38400/50000 (77%)]\tLoss: 0.548269 Accuracy: 80.81%\n",
            "\n",
            "Test set: Average loss: 0.4714, Accuracy: 8472/10000 (84.72%)\n",
            "\n",
            "Epoch: 73 [0/50000 (0%)]\tLoss: 0.503484 Accuracy: 80.47%\n",
            "Epoch: 73 [12800/50000 (26%)]\tLoss: 0.540942 Accuracy: 81.10%\n",
            "Epoch: 73 [25600/50000 (51%)]\tLoss: 0.543741 Accuracy: 81.05%\n",
            "Epoch: 73 [38400/50000 (77%)]\tLoss: 0.543938 Accuracy: 81.03%\n",
            "\n",
            "Test set: Average loss: 0.4705, Accuracy: 8468/10000 (84.68%)\n",
            "\n",
            "Epoch: 74 [0/50000 (0%)]\tLoss: 0.582640 Accuracy: 78.12%\n",
            "Epoch: 74 [12800/50000 (26%)]\tLoss: 0.523102 Accuracy: 81.89%\n",
            "Epoch: 74 [25600/50000 (51%)]\tLoss: 0.528690 Accuracy: 81.58%\n",
            "Epoch: 74 [38400/50000 (77%)]\tLoss: 0.531850 Accuracy: 81.45%\n",
            "\n",
            "Test set: Average loss: 0.4522, Accuracy: 8533/10000 (85.33%)\n",
            "\n",
            "Epoch: 75 [0/50000 (0%)]\tLoss: 0.585005 Accuracy: 77.34%\n",
            "Epoch: 75 [12800/50000 (26%)]\tLoss: 0.539014 Accuracy: 81.03%\n",
            "Epoch: 75 [25600/50000 (51%)]\tLoss: 0.541200 Accuracy: 81.01%\n",
            "Epoch: 75 [38400/50000 (77%)]\tLoss: 0.540233 Accuracy: 81.03%\n",
            "\n",
            "Test set: Average loss: 0.4771, Accuracy: 8488/10000 (84.88%)\n",
            "\n",
            "Epoch: 76 [0/50000 (0%)]\tLoss: 0.464378 Accuracy: 79.69%\n",
            "Epoch: 76 [12800/50000 (26%)]\tLoss: 0.526435 Accuracy: 81.68%\n",
            "Epoch: 76 [25600/50000 (51%)]\tLoss: 0.537430 Accuracy: 81.04%\n",
            "Epoch: 76 [38400/50000 (77%)]\tLoss: 0.535812 Accuracy: 81.16%\n",
            "\n",
            "Test set: Average loss: 0.4512, Accuracy: 8586/10000 (85.86%)\n",
            "\n",
            "Epoch: 77 [0/50000 (0%)]\tLoss: 0.655296 Accuracy: 79.69%\n",
            "Epoch: 77 [12800/50000 (26%)]\tLoss: 0.529787 Accuracy: 81.56%\n",
            "Epoch: 77 [25600/50000 (51%)]\tLoss: 0.532670 Accuracy: 81.24%\n",
            "Epoch: 77 [38400/50000 (77%)]\tLoss: 0.535997 Accuracy: 81.15%\n",
            "\n",
            "Test set: Average loss: 0.4852, Accuracy: 8477/10000 (84.77%)\n",
            "\n",
            "Epoch: 78 [0/50000 (0%)]\tLoss: 0.478398 Accuracy: 83.59%\n",
            "Epoch: 78 [12800/50000 (26%)]\tLoss: 0.521764 Accuracy: 81.61%\n",
            "Epoch: 78 [25600/50000 (51%)]\tLoss: 0.531395 Accuracy: 81.25%\n",
            "Epoch: 78 [38400/50000 (77%)]\tLoss: 0.527893 Accuracy: 81.47%\n",
            "\n",
            "Test set: Average loss: 0.4732, Accuracy: 8497/10000 (84.97%)\n",
            "\n",
            "Epoch: 79 [0/50000 (0%)]\tLoss: 0.529001 Accuracy: 83.59%\n",
            "Epoch: 79 [12800/50000 (26%)]\tLoss: 0.524207 Accuracy: 81.71%\n",
            "Epoch: 79 [25600/50000 (51%)]\tLoss: 0.525916 Accuracy: 81.62%\n",
            "Epoch: 79 [38400/50000 (77%)]\tLoss: 0.523792 Accuracy: 81.72%\n",
            "\n",
            "Test set: Average loss: 0.4805, Accuracy: 8476/10000 (84.76%)\n",
            "\n",
            "Epoch: 80 [0/50000 (0%)]\tLoss: 0.431933 Accuracy: 86.72%\n",
            "Epoch: 80 [12800/50000 (26%)]\tLoss: 0.519777 Accuracy: 81.65%\n",
            "Epoch: 80 [25600/50000 (51%)]\tLoss: 0.518246 Accuracy: 81.79%\n",
            "Epoch: 80 [38400/50000 (77%)]\tLoss: 0.525658 Accuracy: 81.54%\n",
            "\n",
            "Test set: Average loss: 0.4534, Accuracy: 8556/10000 (85.56%)\n",
            "\n",
            "Epoch: 81 [0/50000 (0%)]\tLoss: 0.615498 Accuracy: 74.22%\n",
            "Epoch: 81 [12800/50000 (26%)]\tLoss: 0.507730 Accuracy: 82.22%\n",
            "Epoch: 81 [25600/50000 (51%)]\tLoss: 0.516329 Accuracy: 81.98%\n",
            "Epoch: 81 [38400/50000 (77%)]\tLoss: 0.518810 Accuracy: 81.83%\n",
            "\n",
            "Test set: Average loss: 0.4681, Accuracy: 8504/10000 (85.04%)\n",
            "\n",
            "Epoch: 82 [0/50000 (0%)]\tLoss: 0.620956 Accuracy: 73.44%\n",
            "Epoch: 82 [12800/50000 (26%)]\tLoss: 0.521812 Accuracy: 81.21%\n",
            "Epoch: 82 [25600/50000 (51%)]\tLoss: 0.518848 Accuracy: 81.42%\n",
            "Epoch: 82 [38400/50000 (77%)]\tLoss: 0.520521 Accuracy: 81.41%\n",
            "\n",
            "Test set: Average loss: 0.4638, Accuracy: 8519/10000 (85.19%)\n",
            "\n",
            "Epoch: 83 [0/50000 (0%)]\tLoss: 0.588578 Accuracy: 78.12%\n",
            "Epoch: 83 [12800/50000 (26%)]\tLoss: 0.510310 Accuracy: 82.05%\n",
            "Epoch: 83 [25600/50000 (51%)]\tLoss: 0.522357 Accuracy: 81.67%\n",
            "Epoch: 83 [38400/50000 (77%)]\tLoss: 0.522216 Accuracy: 81.78%\n",
            "\n",
            "Test set: Average loss: 0.4725, Accuracy: 8500/10000 (85.00%)\n",
            "\n",
            "Epoch: 84 [0/50000 (0%)]\tLoss: 0.473920 Accuracy: 84.38%\n",
            "Epoch: 84 [12800/50000 (26%)]\tLoss: 0.513986 Accuracy: 81.79%\n",
            "Epoch: 84 [25600/50000 (51%)]\tLoss: 0.515327 Accuracy: 81.93%\n",
            "Epoch: 84 [38400/50000 (77%)]\tLoss: 0.516034 Accuracy: 81.84%\n",
            "\n",
            "Test set: Average loss: 0.4691, Accuracy: 8477/10000 (84.77%)\n",
            "\n",
            "Epoch: 85 [0/50000 (0%)]\tLoss: 0.558463 Accuracy: 78.12%\n",
            "Epoch: 85 [12800/50000 (26%)]\tLoss: 0.520466 Accuracy: 82.01%\n",
            "Epoch: 85 [25600/50000 (51%)]\tLoss: 0.521177 Accuracy: 82.01%\n",
            "Epoch: 85 [38400/50000 (77%)]\tLoss: 0.515516 Accuracy: 82.11%\n",
            "\n",
            "Test set: Average loss: 0.4684, Accuracy: 8540/10000 (85.40%)\n",
            "\n",
            "Epoch: 86 [0/50000 (0%)]\tLoss: 0.484202 Accuracy: 80.47%\n",
            "Epoch: 86 [12800/50000 (26%)]\tLoss: 0.517810 Accuracy: 81.59%\n",
            "Epoch: 86 [25600/50000 (51%)]\tLoss: 0.519473 Accuracy: 81.55%\n",
            "Epoch: 86 [38400/50000 (77%)]\tLoss: 0.517626 Accuracy: 81.64%\n",
            "\n",
            "Test set: Average loss: 0.4507, Accuracy: 8565/10000 (85.65%)\n",
            "\n",
            "Epoch: 87 [0/50000 (0%)]\tLoss: 0.576280 Accuracy: 78.12%\n",
            "Epoch: 87 [12800/50000 (26%)]\tLoss: 0.522848 Accuracy: 81.85%\n",
            "Epoch: 87 [25600/50000 (51%)]\tLoss: 0.517338 Accuracy: 81.96%\n",
            "Epoch: 87 [38400/50000 (77%)]\tLoss: 0.515420 Accuracy: 82.07%\n",
            "\n",
            "Test set: Average loss: 0.4750, Accuracy: 8513/10000 (85.13%)\n",
            "\n",
            "Epoch: 88 [0/50000 (0%)]\tLoss: 0.486747 Accuracy: 83.59%\n",
            "Epoch: 88 [12800/50000 (26%)]\tLoss: 0.491001 Accuracy: 82.53%\n",
            "Epoch: 88 [25600/50000 (51%)]\tLoss: 0.506696 Accuracy: 81.99%\n",
            "Epoch: 88 [38400/50000 (77%)]\tLoss: 0.509990 Accuracy: 81.94%\n",
            "\n",
            "Test set: Average loss: 0.4672, Accuracy: 8530/10000 (85.30%)\n",
            "\n",
            "Epoch: 89 [0/50000 (0%)]\tLoss: 0.404513 Accuracy: 85.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQ67OntBAzOi"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}